{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rRd71b6bEMc"
      },
      "source": [
        "# Sheet 3 - Sergio Amortegui, Juan David Forero, Navid Esteban Mejia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWlyFXVWbEMe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wbWg6DEJkdig"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numbers\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPVdM8tDkdih"
      },
      "source": [
        "## Exercise 2\n",
        "Use the credits.csv for all the tasks in this exercise (iris datasets\n",
        "only for some unit tests). You might stumble over some difficulties and may want to check\n",
        "the replace and fillna function of Pandas data frames to get rid of missing values. Do not\n",
        "use the get_dummies, digitize, cut, crosstab (or any similar) function in this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1md1ctlkdii"
      },
      "source": [
        "### 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gY-Iux5kdii"
      },
      "source": [
        "#### a)\n",
        "Write a function binarizeCategoricalAttributeVector(column) that takes a\n",
        "categorical attribute vector (do not assume a categorical Pandas type) and re-\n",
        "turns an n ×m dimensional numpy array, where m is the number of categorical\n",
        "values occurring in the column and n is the length of the original column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksrDfHLtkdij"
      },
      "outputs": [],
      "source": [
        "def binarizeCategoricalAttributeVector(column):\n",
        "    n = len(column)\n",
        "    m = set(column)\n",
        "    itemIndex = {}\n",
        "    count = 0\n",
        "\n",
        "    for item in m:\n",
        "      itemIndex[item] = count\n",
        "      count += 1\n",
        "\n",
        "    arr = np.zeros((n,len(m)), dtype=int)\n",
        "    for index, item in enumerate(column):\n",
        "      arr[index][itemIndex[item]] = 1\n",
        "\n",
        "    return arr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRihha3tkdij"
      },
      "source": [
        "#### b)\n",
        "Then write a function getCategoricalAttributes(df) that returns a list of column names of a Pandas DataFrame that contain non-numeric values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmaMdJjyuqjf"
      },
      "outputs": [],
      "source": [
        "def getCategoricalAttributes(df):\n",
        "  return(df.select_dtypes(exclude=['int64','float64']).columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF7f39Izkdik"
      },
      "source": [
        "#### c)\n",
        "Finally, write a function readFrameAsMatrix(df) to convert a given DataFrame\n",
        "into a purely numeric nd array such that each categorical attribute with m values\n",
        "is converted into m binary attributes (columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-i3osP5kdik"
      },
      "outputs": [],
      "source": [
        "def readFrameAsMatrix(df):\n",
        "  # use getCategoricalAttributes and binarize to complete de point\n",
        "  return df.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_dTdkrZkdik"
      },
      "source": [
        "#### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiVLz_Cnkdil"
      },
      "outputs": [],
      "source": [
        "def check_column_conversion(column):\n",
        "    M = binarizeCategoricalAttributeVector(column)\n",
        "    vals = list(np.unique(M))\n",
        "    sorted(vals)\n",
        "    print(\"-----------------\\nBinarization check\\n-----------------\")\n",
        "    print(\"Dimension check: \" + (\"OK\" if M.shape == (len(column), len(np.unique(column))) else \"FAIL\"))\n",
        "    print(\"Occurring values: \" + (\"OK\" if vals == [0, 1] else \"FAIL (there should only be 0s and 1s in the output.)\"))\n",
        "    print(\"Coherence: \" + (\"OK\" if all(np.sum(M, axis=1) == np.ones(len(column))) else \"FAIL (all rows must sum up to 1)\"))\n",
        "\n",
        "def check_category_detection(df, expectedcols):\n",
        "    print(\"-----------------\\nCheck of category detection\\n-----------------\")\n",
        "    act = getCategoricalAttributes(df)\n",
        "    missing = [c for c in expectedcols if not c in act]\n",
        "    unexpected = [c for c in act if not c in expectedcols]\n",
        "    print(\"Categorical attribute detection: \" + (\"OK\" if len(missing) + len(unexpected) == 0 else \"FAIL (undetected columns: \" + str(missing) + \", wrongly detected columns: \" + str(unexpected) + \")\"))\n",
        "    \n",
        "def check_frame_conversion(df, num_expected_columns):\n",
        "    print(\"-----------------\\nConversion check for data frames\\n-----------------\")\n",
        "    A = readFrameAsMatrix(df)\n",
        "    print(\"Outer Type check: \" + (\"OK\" if type(A) == np.ndarray else \"FAIL (not a numpy array but \" + str(type(A)) + \")\"))\n",
        "    print(\"Inner Type check: \" + (\"OK\" if A.dtype in [float, np.float32, np.float64] else \"FAIL (dtype of matrix should be something numeric like float and not \" + str(A.dtype) + \")\"))\n",
        "    print(\"Dimensionality check: \" + (\"OK\" if len(A) == len(df) and A.shape[1] == num_expected_columns else \"FAIL (expected shape \" + str(len(df)) + \" x \" + str(num_expected_columns) + \", but observed shape \" + str(len(A)) + \" x \" + str(A.shape[1]) + \")\"))\n",
        "\n",
        "\n",
        "## unit test for conversion functions\n",
        "dfCreditTest = pd.read_csv(\"credits.csv\")\n",
        "check_column_conversion(dfCreditTest.values[:,1])\n",
        "check_category_detection(dfCreditTest, ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'class'])\n",
        "check_frame_conversion(dfCreditTest, 110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxVmQJ85kdil"
      },
      "source": [
        "### 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J80CD4wkdil"
      },
      "outputs": [],
      "source": [
        "def test_discretization(column, thresholds, names, expected):\n",
        "    conv = discretizeBasedOnThresholds(column, thresholds, names)\n",
        "    print(\"Conversion test: \" + (\"OK\" if len(conv) == len(expected) and all(conv == expected) else \"FAIL (expected \\\"\" + str(expected) +\"\\\" but observed \\\"\" + str(conv) + \"\\\")\"))\n",
        "    \n",
        "def test_equal_length_discretization(arr, k, expected):\n",
        "    act = discretizeEqualLength(arr, k)\n",
        "    print (\"Equal Length Discretization: \" + (\"OK\" if all(act == expected) else \"FAIL\"))\n",
        "    \n",
        "def test_equal_count_discretization(arr, k, expected):\n",
        "    act = discretizeEqualFrequency(arr, k)\n",
        "    print (\"Equal Count Discretization: \" + (\"OK\" if all(act == expected) else \"FAIL\"))\n",
        "\n",
        "# reproduce results from the lecture\n",
        "dfIrisTest = pd.read_csv(\"iris.csv\")\n",
        "test_discretization(dfIris.values[:,0], [5.2, 6.1, 7], [\"very short\", \"short\", \"long\", \"very long\"], [\"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"short\", \"very short\", \"very short\", \"very short\", \"very short\", \"short\", \"very short\", \"very short\", \"very short\", \"short\", \"short\", \"short\", \"very short\", \"short\", \"very short\", \"short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"short\", \"very short\", \"short\", \"very short\", \"very short\", \"short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"very short\", \"short\", \"very short\", \"long\", \"long\", \"long\", \"short\", \"long\", \"short\", \"long\", \"very short\", \"long\", \"very short\", \"very short\", \"short\", \"short\", \"short\", \"short\", \"long\", \"short\", \"short\", \"long\", \"short\", \"short\", \"short\", \"long\", \"short\", \"long\", \"long\", \"long\", \"long\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"long\", \"long\", \"short\", \"short\", \"short\", \"short\", \"short\", \"very short\", \"short\", \"short\", \"short\", \"long\", \"very short\", \"short\", \"long\", \"short\", \"very long\", \"long\", \"long\", \"very long\", \"very short\", \"very long\", \"long\", \"very long\", \"long\", \"long\", \"long\", \"short\", \"short\", \"long\", \"long\", \"very long\", \"very long\", \"short\", \"long\", \"short\", \"very long\", \"long\", \"long\", \"very long\", \"long\", \"short\", \"long\", \"very long\", \"very long\", \"very long\", \"long\", \"long\", \"short\", \"very long\", \"long\", \"long\", \"short\", \"long\", \"long\", \"long\", \"short\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"short\"])\n",
        "test_discretization(dfIris.values[:,1], [2.8, 3.6], [\"short\", \"medium\", \"long\"], [\"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"long\", \"medium\", \"medium\", \"medium\", \"medium\", \"long\", \"medium\", \"medium\", \"medium\", \"long\", \"long\", \"long\", \"medium\", \"long\", \"long\", \"medium\", \"long\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"long\", \"long\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"short\", \"medium\", \"medium\", \"long\", \"medium\", \"long\", \"medium\", \"long\", \"medium\", \"medium\", \"medium\", \"medium\", \"short\", \"short\", \"short\", \"medium\", \"short\", \"medium\", \"short\", \"short\", \"medium\", \"short\", \"medium\", \"medium\", \"medium\", \"medium\", \"short\", \"short\", \"short\", \"medium\", \"short\", \"short\", \"short\", \"medium\", \"medium\", \"short\", \"medium\", \"medium\", \"short\", \"short\", \"short\", \"short\", \"short\", \"medium\", \"medium\", \"medium\", \"short\", \"medium\", \"short\", \"short\", \"medium\", \"short\", \"short\", \"short\", \"medium\", \"medium\", \"medium\", \"short\", \"short\", \"medium\", \"short\", \"medium\", \"medium\", \"medium\", \"medium\", \"short\", \"medium\", \"short\", \"medium\", \"medium\", \"short\", \"medium\", \"short\", \"short\", \"medium\", \"medium\", \"long\", \"short\", \"short\", \"medium\", \"short\", \"short\", \"short\", \"medium\", \"medium\", \"short\", \"medium\", \"short\", \"medium\", \"short\", \"long\", \"short\", \"short\", \"short\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"short\", \"medium\", \"medium\", \"medium\", \"short\", \"medium\", \"medium\", \"medium\"])\n",
        "test_equal_length_discretization(dfIrisTest.values[:,0], 4, np.array([\"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c1\", \"c0\", \"c0\", \"c0\", \"c0\", \"c1\", \"c0\", \"c0\", \"c0\", \"c1\", \"c1\", \"c1\", \"c0\", \"c1\", \"c0\", \"c1\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c1\", \"c0\", \"c1\", \"c0\", \"c0\", \"c1\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c1\", \"c0\", \"c2\", \"c2\", \"c2\", \"c1\", \"c2\", \"c1\", \"c2\", \"c0\", \"c2\", \"c0\", \"c0\", \"c1\", \"c1\", \"c1\", \"c1\", \"c2\", \"c1\", \"c1\", \"c2\", \"c1\", \"c1\", \"c1\", \"c2\", \"c1\", \"c2\", \"c2\", \"c2\", \"c2\", \"c1\", \"c1\", \"c1\", \"c1\", \"c1\", \"c1\", \"c1\", \"c1\", \"c2\", \"c2\", \"c1\", \"c1\", \"c1\", \"c1\", \"c1\", \"c0\", \"c1\", \"c1\", \"c1\", \"c2\", \"c0\", \"c1\", \"c2\", \"c1\", \"c3\", \"c2\", \"c2\", \"c3\", \"c0\", \"c3\", \"c2\", \"c3\", \"c2\", \"c2\", \"c2\", \"c1\", \"c1\", \"c2\", \"c2\", \"c3\", \"c3\", \"c1\", \"c2\", \"c1\", \"c3\", \"c2\", \"c2\", \"c3\", \"c2\", \"c1\", \"c2\", \"c3\", \"c3\", \"c3\", \"c2\", \"c2\", \"c1\", \"c3\", \"c2\", \"c2\", \"c1\", \"c2\", \"c2\", \"c2\", \"c1\", \"c2\", \"c2\", \"c2\", \"c2\", \"c2\", \"c2\", \"c1\"]))\n",
        "test_equal_count_discretization(dfIrisTest.values[:,0], 4, np.array([\"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c1\", \"c0\", \"c0\", \"c0\", \"c0\", \"c1\", \"c0\", \"c0\", \"c0\", \"c1\", \"c1\", \"c1\", \"c0\", \"c1\", \"c0\", \"c1\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c1\", \"c1\", \"c0\", \"c0\", \"c1\", \"c1\", \"c1\", \"c0\", \"c0\", \"c1\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c0\", \"c1\", \"c0\", \"c3\", \"c2\", \"c3\", \"c1\", \"c3\", \"c1\", \"c2\", \"c0\", \"c3\", \"c1\", \"c0\", \"c2\", \"c2\", \"c2\", \"c1\", \"c3\", \"c1\", \"c1\", \"c2\", \"c1\", \"c2\", \"c2\", \"c2\", \"c2\", \"c2\", \"c3\", \"c3\", \"c3\", \"c2\", \"c1\", \"c1\", \"c1\", \"c1\", \"c2\", \"c1\", \"c2\", \"c3\", \"c2\", \"c1\", \"c1\", \"c1\", \"c2\", \"c1\", \"c0\", \"c1\", \"c1\", \"c1\", \"c2\", \"c0\", \"c1\", \"c2\", \"c1\", \"c3\", \"c2\", \"c3\", \"c3\", \"c0\", \"c3\", \"c3\", \"c3\", \"c3\", \"c2\", \"c3\", \"c1\", \"c1\", \"c2\", \"c3\", \"c3\", \"c3\", \"c2\", \"c3\", \"c1\", \"c3\", \"c2\", \"c3\", \"c3\", \"c2\", \"c2\", \"c2\", \"c3\", \"c3\", \"c3\", \"c2\", \"c2\", \"c2\", \"c3\", \"c2\", \"c2\", \"c2\", \"c3\", \"c3\", \"c3\", \"c1\", \"c3\", \"c3\", \"c3\", \"c2\", \"c3\", \"c2\", \"c2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awEQOODtkdim"
      },
      "source": [
        "### 3\n",
        "In this exercise we want to check the independence of categorical attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwzDPC43ZI9J"
      },
      "source": [
        "#### a)\n",
        "Write a function getContingencyTable(M) that receives a 2D numpy array with\n",
        "two columns and computes a table containing the absolute observed frequenties of\n",
        "the pairs of occuring values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Wy0r72riIKl"
      },
      "outputs": [],
      "source": [
        "def getContingencyTable(M):\n",
        "  M_transposed = M.T\n",
        "  M_zipped = list(zip(M_transposed[0], M_transposed[1]))\n",
        "  y_categories, x_categories = (set(M_transposed[0]), set(M_transposed[1]))\n",
        "  products = list(itertools.product(y_categories, x_categories))\n",
        "  categories = dict(zip(products, np.array([0] * len(products))))\n",
        "  y_index = {item:index for index, item in enumerate(y_categories)} \n",
        "  x_index = {item:index for index, item in enumerate(x_categories)}\n",
        "  result = np.zeros((len(y_categories), len(x_categories)), dtype='int')\n",
        "  for item in M_zipped:\n",
        "    categories[item] = categories[item] + 1\n",
        "  for categorie in categories.items():\n",
        "    result[y_index[categorie[0][0]]][x_index[categorie[0][1]]] = categorie[1]\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YieWSEP4ZI9J"
      },
      "source": [
        "#### b)\n",
        "Write a function computeExpectedOccurrences(ct) that receives a contingency\n",
        "table and computes a table containing, for each pair of values, the number of\n",
        "occurences one would expect given independency of the attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bCHtzHf7ZI9J"
      },
      "outputs": [],
      "source": [
        "def computeExpectedOccurrences(ct):\n",
        "    total = sum(np.sum(ct, axis=0))\n",
        "    ct_list = ct.tolist()\n",
        "    ct_list_transposed = np.transpose(ct_list).tolist()\n",
        "\n",
        "    result = ct_list.copy()\n",
        "\n",
        "    for i in range(len(ct_list)):\n",
        "        row = np.sum(ct_list[i], dtype=int)\n",
        "        for j in range(len(ct_list_transposed)):\n",
        "            column = np.sum(ct_list_transposed[j], dtype=int)\n",
        "\n",
        "            result[i][j] = ( row * column ) / total\n",
        "\n",
        "    return np.array(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1NKxoVWbEMm"
      },
      "source": [
        "#### c)\n",
        "Write a function computeChiSquare(M) that receives a 2D numpy array with two\n",
        "discrete columns and computes the χ2score of the two attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uZjHwcsJbEMn"
      },
      "outputs": [],
      "source": [
        "def computeChiSquare(M):\n",
        "    O = getContingencyTable(M)\n",
        "    E = computeExpectedOccurrences(O)\n",
        "    X2 = 0\n",
        "    \n",
        "    for i in range(len(O)):\n",
        "        for j in range(len(O[0])):\n",
        "            o = O[i][j]\n",
        "            e = E[i][j]\n",
        "            X2 += ( (o - e ) ** 2 ) / e\n",
        "\n",
        "    return X2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LWZsTwUZQIa"
      },
      "source": [
        "#### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "hdbUfjW6kdim"
      },
      "outputs": [],
      "source": [
        "def test_contingency_table(col1, col2):\n",
        "    print(\"-----------------\\nCheck for Contingency Table\\n-----------------\")\n",
        "    M = np.array([col1, col2]).T\n",
        "    ct = getContingencyTable(M)\n",
        "    s2 = np.sum(ct, axis=0)\n",
        "    s1 = np.sum(ct, axis=1)\n",
        "    print(\"Dimensionality: \" + (\"OK\" if len(s1) == len(np.unique(col1)) and len(s2) == len(np.unique(col2)) else \"FAIL (expected dimension \" + str(len(np.unique(col1))) + \" x \" + str(len(np.unique(col2))) + \" but observed \" + str(len(s1)) + \" x \" + str(len(s2)) + \")\"))\n",
        "    print(\"Sum 1: \" + (\"OK\" if sum(s1) == len(col1) else \"FAIL\"))\n",
        "    print(\"Sum 2: \" + (\"OK\" if sum(s2) == len(col2) else \"FAIL\"))\n",
        "    \n",
        "def test_expected_table(ct):\n",
        "    print(\"-----------------\\nCheck for Expected Count Table\\n-----------------\")\n",
        "    cs1 = np.sum(ct, axis=1)\n",
        "    cs2 = np.sum(ct, axis=0)\n",
        "    et = computeExpectedOccurrences(ct)\n",
        "    s2 = np.sum(et, axis=0)\n",
        "    s1 = np.sum(et, axis=1)\n",
        "    print(\"Dimensionality: \" + (\"OK\" if et.shape == ct.shape else \"FAIL (expected dimension \" + str(ct.shape[0]) + \" x \" + str(ct.shape[1]) + \" but observed \" + str(et.shape[0]) + \" x \" + str(et.shape[1]) + \")\"))\n",
        "    print(\"Sum 1: \" + (\"OK\" if len(cs1) == len(s1) and all(np.isclose(cs1, s1)) else \"FAIL\"))\n",
        "    print(\"Sum 2: \" + (\"OK\" if len(cs2) == len(s2) and all(np.isclose(cs2, s2)) else \"FAIL\"))\n",
        "    \n",
        "def test_chi2(M, expected):\n",
        "    print(\"-----------------\\nChi²-Test\\n-----------------\")\n",
        "    chi2 = computeChiSquare(M)\n",
        "    print (\"Chi²: \" + (\"OK\" if np.round(chi2, 2) == expected else \"FAIL\"))\n",
        "\n",
        "    \n",
        "# reproduce results from the lecture\n",
        "dfIrisDisc = pd.read_csv(\"iris_discretized_projected.csv\")\n",
        "test_contingency_table(dfIrisDisc.values[:,0], dfIrisDisc.values[:,1])\n",
        "test_expected_table(getContingencyTable(np.array([dfIrisDisc.values[:,0], dfIrisDisc.values[:,1]]).T))\n",
        "test_chi2(np.array([dfIrisDisc.values[:,0], dfIrisDisc.values[:,1]]).T, 21.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om6TikjJbEMo"
      },
      "source": [
        "#### d)\n",
        "Write a function checkIndependence(df, c1, c2, alpha) that receives a Pandas\n",
        "DataFrame and the names of two columns and that returns true iff the indepen-\n",
        "dence hypothesis is sustained in a χ2test (considering the appropriate degree of\n",
        "freedom) for a given confidence threshold α for the p-value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "gRLgrvFHbEMo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157.5469673873229\n",
            "149 791.7225565439824\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from scipy.stats import chi2\n",
        "import scipy.stats as stats\n",
        "data = pd.read_csv('iris.csv')\n",
        "\n",
        "def checkIndepence(df, c1,c2, alpha):\n",
        "    M = df[[c1,c2]].values\n",
        "    # O = getContingencyTable(M)\n",
        "    # E = computeExpectedOccurrences(O)\n",
        "    X2 = computeChiSquare(M) \n",
        "    q = (len(M) - 1) * (len(M[0]) - 1)\n",
        "\n",
        "    ppf = stats.chi2.ppf(alpha, q)\n",
        "    print(ppf)\n",
        "    print(q, X2)\n",
        "\n",
        "    return True if X2 < ppf else False\n",
        "\n",
        "\n",
        "checkIndepence(data, 'sepal_length', 'sepal_width', 0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85l4yc1ZbEMo"
      },
      "source": [
        "#### e)\n",
        "Then check independence hypothesis for all pairs of categorical variables of the\n",
        "credit dataset. Plot the χ2curve for every pair of categorical variables with the\n",
        "respective (given) critical point (we assume α = 0.01).\n",
        "Are there pairs of independent variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmhXvdA8bEMo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bi2022_1_sheet3_template.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
